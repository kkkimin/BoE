import torch
from unsloth import FastLanguageModel
import json
import glob
from datasets import Dataset
from datasets import DatasetDict
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
import evaluate

max_seq_length = 512  # 1024 -> 512 수정
dtype = None
load_in_4bit = True

# 1. 모델과 토크나이저 로드
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="davidkim205/ko-gemma-2-9b-it",
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=True,
    device_map="auto",         # 자동 메모리 관리
)

# 2. PEFT 모델 설정
model = FastLanguageModel.get_peft_model(
    model,
    r=8,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj", ],
    lora_alpha=8,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=42,
    use_rslora=False,
    loftq_config=None,
)

prompt = """<start_of_turn>user
다음 문장을 점역해줘.
{}
<end_of_turn><start_of_turn>model
{}"""


def formatting_prompts_func(examples):
    inputs = examples["source"]
    outputs = examples["target"]
    texts = []
    for input, output in zip(inputs, outputs):
        text = prompt.format(input, output)
        texts.append(text + "<end_of_turn><eos>")
    return {"text": texts, }


def load_data():
    src_texts = []
    tgt_texts = []

    json_files = glob.glob('./data/*.json')

    for json_file in json_files:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for item in data.get('parallel', []):
                src_texts.append(item['source'])
                tgt_texts.append(item['target'])

    dataset = Dataset.from_dict({"source": src_texts, "target": tgt_texts})

    return dataset


# 3. 데이터 로드 및 전처리
data = load_data()

train_test_split = data.train_test_split(train_size=0.7, test_size=0.3, shuffle=True)
eval_test_split = train_test_split['test'].train_test_split(train_size=0.5, test_size=0.5, shuffle=True)

dataset_split = DatasetDict({
    'train': train_test_split['train'],
    'eval': eval_test_split['train'],
    'test': eval_test_split['test']
})

dataset_split['train'] = dataset_split['train'].map(formatting_prompts_func, batched=True)


# 4. TrainingArguments 설정
training_args =TrainingArguments(
    per_device_train_batch_size=4,  # 8-> 4 수정
    per_device_eval_batch_size=4,   # 8-> 4 수정
    gradient_accumulation_steps=8,  # 4-> 8 수정
    warmup_steps=5,
    num_train_epochs=3,
    eval_strategy="steps",   # 평가 주기 설정
    eval_steps=500,
    save_strategy="steps",   # 저장 주기 설정
    save_steps=500, 
    save_total_limit=3,      # 최대 3개의 체크포인트만 저장
    learning_rate=2e-4,
    fp16=not is_bfloat16_supported(),
    bf16=is_bfloat16_supported(),
    logging_steps=100,
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="linear",
    seed=42,
    gradient_checkpointing=True,  # 메모리 사용량 감소
    max_grad_norm=1.0,           # 그래디언트 클리핑
    dataloader_num_workers=2,    # 데이터 로딩 최적화
    remove_unused_columns=True,  # 불필요한 컬럼 제거
    output_dir="outputs",        # 체크포인트 저장될 디렉토리  
    report_to="none",
    # 체크포인트에서 학습 재개를 위한 설정
    resume_from_checkpoint="outputs/checkpoint-1500",  # 예시임!!! (실제 체크포인트 확인!) # 체크포인트 확인방법 : ls outputs/ 
)

# 5. Trainer 초기화 및 학습 재개
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset_split['train'],
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    packing=False,
    args=training_args,
)

# 6. (특정 체크포인트에서) 학습 재개
trainer_stats = trainer.train(resume_from_checkpoint="outputs/checkpoint-1500")  # 예시임!!! (실제 체크포인트 확인필요)
# 가장 최근 체크포인트에서 재개하고 싶다면
# trainer_stats = trainer.train(resume_from_checkpoint=True)

FastLanguageModel.for_inference(model)

text_ref = []
text_gen = []
batch_size = 2  # 작은 배치 사이즈로 추론

for i in range(0, len(dataset_split['test']), batch_size):
    batch = dataset_split['test'][i:i+batch_size]
    input_texts = [prompt.format(item['source'], "") for item in batch]
    
    # 입력 처리
    inputs = tokenizer(input_texts, 
                      return_tensors="pt",
                      padding=True,
                      truncation=True,
                      max_length=max_seq_length).to("cuda")
    
    # 생성
    with torch.no_grad():  # 메모리 사용량 감소
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,  # max_new_tokens 감소
            use_cache=True,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    # 결과 처리
    generated_texts = tokenizer.batch_decode(outputs)
    for j, generated in enumerate(generated_texts):
        if i+j < len(dataset_split['test']):
            original_input = input_texts[j]
            generated = generated[len(original_input):].replace('<end_of_turn><eos>', '')
            text_ref.append(batch[j]['target'])
            text_gen.append(generated)
    
    # GPU 캐시 정리
    torch.cuda.empty_cache()